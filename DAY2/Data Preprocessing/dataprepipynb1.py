# -*- coding: utf-8 -*-
"""DataPrepipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BQxIDbJiCrUDO5ZJLPr9HGCz4f-CWOFW
"""

import pandas as pd
df=pd.read_csv("/content/FullDataset.csv")

# ===========================================
# ðŸ“Œ Recall Events Preprocessing with NLP + Dates + Finalized Class
# ===========================================
import pandas as pd
import numpy as np
import re
from datetime import datetime

# =========================
# CONFIG / KEYWORDS
# =========================

ACTION_PATTERNS = {
    r"\brecall(s)?\b": "Recall",
    r"\bfield\s*safety\s*notice\b|\bfsn\b": "Field Safety Notice",
    r"\bsafety\s*alert\b|\balert\b|\bwarning\b|\badvis(ory|e)\b": "Safety Alert",
}

MEANINGFUL_KEYWORDS = [
    "defect","malfunction","injur","death","risk","hazard","adverse","corrective",
    "failure","nonconform","non-conform","contamination","infection","fire",
    "electric","shock","overheat","dose","label","misuse","break","fracture","recall"
]

CLASS_PATTERNS = [
    (r"\bclass\s*i\b|\bclass\s*1\b|\bci\b|\bclass\s*i:\b", "Class I"),
    (r"\bclass\s*ii\b|\bclass\s*2\b|\bcii\b|\bclass\s*ii:\b", "Class II"),
    (r"\bclass\s*iii\b|\bclass\s*3\b|\bciii\b|\bclass\s*iii:\b", "Class III"),
]

CAUSE_PATTERNS = {
    r"design|specification": "Design Issue",
    r"manufactur|production|assembly|process": "Manufacturing Issue",
    r"label|ifu|instructions|mislabel": "Labeling Issue",
    r"software|firmware|algorithm|code": "Software Issue",
    r"packag": "Packaging Issue",
    r"contamin|steril|clean": "Contamination / Sterility",
    r"user\s*error|use\s*error|training": "Use Error / Training",
    r"supplier|component": "Supplier/Component Issue",
    r"quality\s*system|documentation": "Quality System",
}

SEVERITY_KEYWORDS = {
    "Severe": ["death", "life-threatening", "fire", "shock", "permanent", "serious injur"],
    "Moderate": ["injur", "malfunction", "overheat", "delay", "incorrect", "failure"],
    "Minor": ["label", "packaging", "typo", "clarification", "cosmetic"],
}

# =========================
# HELPERS
# =========================

def _safe_lower(x):
    return x.lower() if isinstance(x, str) else ""

def parse_date(series: pd.Series) -> pd.Series:
    parsed = pd.to_datetime(series, errors="coerce", infer_datetime_format=True, utc=False)
    return parsed.dt.strftime("%Y-%m-%d")

def normalize_classification(text: str):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    t = text.lower()
    for pat, label in CLASS_PATTERNS:
        if re.search(pat, t):
            return label
    return np.nan

def infer_action_from_text(action, summary, notes):
    text = " ".join([_safe_lower(action), _safe_lower(summary), _safe_lower(notes)]).strip()
    for pat, label in ACTION_PATTERNS.items():
        if re.search(pat, text): return label
    meaningful = (len(_safe_lower(summary)) >= 20) or any(k in text for k in MEANINGFUL_KEYWORDS)
    return "Meaningful data" if meaningful else "Meaningless data"

def normalize_cause(text: str):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    t = text.lower()
    for pat, label in CAUSE_PATTERNS.items():
        if re.search(pat, t): return label
    return "Other / Unspecified"

def normalize_number(series: pd.Series) -> pd.Series:
    s = series.fillna("").astype(str)
    s = s.str.replace(r"[\s\-_/]+", "", regex=True).str.upper()
    return s.replace({"": np.nan})

def extract_severity(text: str) -> str:
    """Assign severity level from keywords in text"""
    if not isinstance(text, str):
        return "Unknown"
    t = text.lower()
    for sev, kws in SEVERITY_KEYWORDS.items():
        if any(k in t for k in kws):
            return sev
    return "Unknown"

def assign_finalized_class(row):
    """Final classification using multiple fields"""
    # 1. Direct classification if available
    ac = str(row.get("action_classification", "")).lower()
    if "class i" in ac: return "Class I"
    if "class ii" in ac: return "Class II"
    if "class iii" in ac: return "Class III"

    # 2. Use severity keywords
    summary_reason = " ".join([
        str(row.get("action_summary", "")),
        str(row.get("reason", "")),
        str(row.get("action", ""))
    ]).lower()

    if any(k in summary_reason for k in ["death","life-threatening","serious injur","contamination","fire","explosion"]):
        return "Class I"
    if any(k in summary_reason for k in ["malfunction","temporary injur","incorrect","failure","overdose","underdose","mislabel"]):
        return "Class II"
    if any(k in summary_reason for k in ["label","packaging","clerical","printing error","administrative"]):
        return "Class III"

    # 3. Fallback: status
    if str(row.get("status", "")).lower() in ["ongoing","active"]:
        return "Class II"

    # 4. Default = Class II
    return "Class II"

# =========================
# MAIN PREPROCESSOR
# =========================

def preprocess_recalls_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # ---- [your existing steps] ----
    expected = [
        "id","action","action_classification","action_level","action_summary","authorities_link","country",
        "create_date","data_notes","date","date_initiated_by_firm","date_posted","date_terminated","date_updated",
        "determined_cause","documents","icij_notes","number","reason","source","status","target_audience",
        "type","uid","uid_hash","url","slug","device_id","created_at","updated_at"
    ]
    for col in expected:
        if col not in df.columns: df[col] = np.nan

    df = df.replace({"": np.nan, "NaN": np.nan, "nan": np.nan, "None": np.nan, "null": np.nan})

    cols_to_flag = ["action","action_classification","action_summary",
                    "create_date","date_initiated_by_firm","date_posted",
                    "date_terminated","date_updated","determined_cause",
                    "documents","authorities_link","number","reason"]
    for col in cols_to_flag:
        df[f"was_missing_{col}"] = df[col].isna().astype("int8")

    df["action"] = df.apply(
        lambda r: r["action"] if pd.notna(r["action"]) else infer_action_from_text(r["action"], r["action_summary"], r["data_notes"]),
        axis=1
    )
    df["action_classification"] = df["action_classification"].apply(normalize_classification).fillna("Unclassified")

    df["action_summary"] = df["action_summary"].fillna(df["action"]).fillna("No Summary")
    df["action_summary"] = df["action_summary"].astype(str).str.replace(r"\s+", " ", regex=True).str.strip()
    df["recall_severity"] = df["action_summary"].apply(extract_severity)

    for dc in ["create_date","date_initiated_by_firm","date_posted","date_terminated","date_updated"]:
        df[dc] = parse_date(df[dc])

    m = df["date_initiated_by_firm"].isna() & df["date_posted"].notna()
    df.loc[m, "date_initiated_by_firm"] = df.loc[m, "date_posted"]

    df.loc[df["date_terminated"].isna(), "status"] = df["status"].fillna("Ongoing")

    today = pd.to_datetime("today").normalize()
    df["date_initiated_by_firm"] = pd.to_datetime(df["date_initiated_by_firm"], errors="coerce")
    df["date_posted"] = pd.to_datetime(df["date_posted"], errors="coerce")
    df["date_terminated"] = pd.to_datetime(df["date_terminated"], errors="coerce")

    df["days_to_posted"] = (df["date_posted"] - df["date_initiated_by_firm"]).dt.days
    df["days_active"] = np.where(df["date_terminated"].notna(),
                                 (df["date_terminated"] - df["date_initiated_by_firm"]).dt.days,
                                 (today - df["date_initiated_by_firm"]).dt.days)

    df["determined_cause"] = df["determined_cause"].apply(normalize_cause).fillna("Cause Not Reported")

    for col in ["documents","authorities_link"]:
        df[col] = df[col].fillna("No Document Available")

    df["number"] = normalize_number(df["number"])
    pseudo = (df["country"].fillna("").astype(str).str.upper().str.replace(r"\s+", "", regex=True) +
              df["create_date"].fillna("").astype(str).str.replace(r"\D", "", regex=True) +
              df["id"].fillna("").astype(str))
    df["number"] = df["number"].fillna(pseudo)

    for tcol in ["data_notes","reason","icij_notes"]:
        df[tcol] = df[tcol].fillna("not reported")
        df[tcol] = df[tcol].astype(str).str.lower().str.replace(r"\s+", " ", regex=True).str.strip()

    df["id"] = df["id"].astype(str).str.strip()
    df = df.drop_duplicates(subset=["id"], keep="first")

    for c in ["action","action_classification","recall_severity","status","determined_cause","country"]:
        df[c] = df[c].astype("category")

    # ---- NEW FINALIZED CLASS ----
    df["finalized_class"] = df.apply(assign_finalized_class, axis=1)

    return df


# =========================
# RUN ON YOUR FILE
# =========================
in_path  = "/content/FullDataset.csv"   # change to your uploaded path
out_path = "/content/FullDataset_preprocessed.csv"

df = pd.read_csv(in_path, dtype=str, keep_default_na=False, na_values=["", "NaN", "nan", "None", "null"])
processed = preprocess_recalls_df(df)

processed.to_csv(out_path, index=False)
print("âœ… Preprocessed file saved at:", out_path)

# Preview
print(processed[["id","action","action_classification","finalized_class","recall_severity","days_to_posted","days_active"]].head(10))
