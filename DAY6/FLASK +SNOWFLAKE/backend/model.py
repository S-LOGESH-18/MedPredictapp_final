# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocgsuyHWAj_oxikrKiMUbD1gMezD7Ibw
"""

# ============================================
# üöÄ Faulty Device Severity Prediction (Colab/RAM-safe)
#   - Multinomial Logistic Regression (saga)
#   - Predict by device_id
# ============================================

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# -----------------------------
# 1) Load CSVs (robust encoding)
# -----------------------------
devices = pd.read_csv("D:/CTS Hackathon/predictapp/backend/devicepreprocessed.csv", encoding="ISO-8859-1", low_memory=False)
manufacturers = pd.read_csv("D:/CTS Hackathon/predictapp/backend/manufacturer.csv", encoding="ISO-8859-1", low_memory=False)
events = pd.read_csv("D:/CTS Hackathon/predictapp/backend/final_preprocessed.csv", encoding="ISO-8859-1", low_memory=False)

# -----------------------------
# 2) Merge: events ‚Üí devices ‚Üí manufacturers
# -----------------------------
df = (
    events.merge(devices, left_on="device_id", right_on="id", how="left", suffixes=("", "_device"))
          .merge(manufacturers, left_on="manufacturer_id", right_on="id", how="left", suffixes=("", "_manufacturer"))
)

# -----------------------------
# 3) Target = action_classification (drop rows with missing target)
# -----------------------------
if "action_classification" not in df.columns:
    raise ValueError("Column 'action_classification' not found. Check the CSV headers.")

df = df[df["action_classification"].notna()].copy()
y = df["action_classification"].astype(str)

# ---------------------------------------------------------
# 4) Keep a SMALL, useful feature set (RAM-friendly choice)
#    (Only columns that typically exist based on your schemas)
# ---------------------------------------------------------
candidate_feature_cols = [
    # from devices.csv
    "classification", "code", "implanted", "number", "quantity_in_commerce", "risk_class", "country",
    # from events.csv
    "type",
    # (Avoid large free-text/date fields to keep memory low)
]

feature_cols = [c for c in candidate_feature_cols if c in df.columns]
if not feature_cols:
    raise ValueError("None of the expected feature columns were found. "
                     "Check your CSVs or add more columns to 'candidate_feature_cols'.")

X = df[feature_cols].copy()

# ---------------------------------
# 5) Split train/test (with stratify)
# ---------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------------------------------
# 6) Preprocess (impute + encode) ‚Üí Logistic Regression
#    - Numeric: median impute
#    - Categorical: most_frequent impute + OrdinalEncode
#    - OrdinalEncoder keeps memory tiny (vs one-hot)
# ----------------------------------------------------
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = [c for c in X.columns if c not in numeric_cols]

numeric_pipe = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    # (No scaling to save memory; you can add StandardScaler if needed)
])

categorical_pipe = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_pipe, numeric_cols),
        ("cat", categorical_pipe, categorical_cols),
    ],
    remainder="drop",
)

logreg = LogisticRegression(
    max_iter=500,
    multi_class="multinomial",
    solver="saga",          # memory-efficient for large/sparse-ish problems
    n_jobs=-1               # use available cores
)

clf = make_pipeline(preprocess, logreg)

# -----------------------------
# 7) Train
# -----------------------------
clf.fit(X_train, y_train)

# -----------------------------
# 8) Evaluate
# -----------------------------
y_pred = clf.predict(X_test)
print("üìä Classification Report")
print(classification_report(y_test, y_pred))
print("\nüßÆ Confusion Matrix")
print(confusion_matrix(y_test, y_pred))

# -------------------------------------------------------
# 9) Helper: predict severity for a given `device_id`
#    (uses the same pipeline so preprocessing stays consistent)
# -------------------------------------------------------
def predict_device_severity(device_id):
    subset = df[df["device_id"] == device_id]
    if subset.empty:
        return f"‚ùå Device ID {device_id} not found in merged data."

    X_new = subset[feature_cols].copy()
    preds = clf.predict(X_new)
    probs = clf.predict_proba(X_new) if hasattr(clf, "predict_proba") else None

    # Take first row in case multiple events exist for same device_id
    pred = preds[0]
    out = f"‚úÖ Predicted class severity for device_id={device_id}: {pred}"
    if probs is not None:
        classes = clf.named_steps["logisticregression"].classes_
        proba_row = probs[0]
        proba_sorted = sorted(zip(classes, proba_row), key=lambda x: -x[1])
        top = ", ".join([f"{c}: {p:.2f}" for c, p in proba_sorted])
        out += f"\n   Probabilities ‚Üí {top}"
    return out

device_input = int(input("Enter a device_id to predict severity: "))
print(predict_device_severity(device_input))